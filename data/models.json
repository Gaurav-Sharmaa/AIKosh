[
  {
    "id": 1,
    "title": "AI Fraud Detection",
    "description": "An AI-powered system using machine learning to detect anomalies in transactional data, enabling real-time fraud detection and reducing financial risks.",
    "about_model": "An intelligent machine learning system designed to detect fraudulent activities in transactional data. This project helps minimize financial fraud by identifying anomalous patterns, even in highly imbalanced datasets, using supervised learning and advanced model tuning techniques.",
    "image_url": null,
    "tags": [
      "AI",
      "Machine Learning"
    ],
    "likes_count": 0,
    "downloads_count": 22,
    "views_count": 104,
    "source_org": "Cyfuture India Pvt Ltd",
    "license": "Apache 2.0",
    "hosted_by": "Cyfuture-AI",
    "model_type": "Machine Learning Model",
    "model_format": "PyTorch",
    "visibility": "Open",
    "source_organization": "Cyfuture India Pvt Ltd",
    "sector": "Other",
    "updated_at": "2025-09-25T11:44:06+05:30",
    "created_by": "Hemant Pal",
    "size": "N.A."
  },
  {
    "id": 2,
    "title": "Knowledge Base",
    "description": "Knowledge Base construction includes tokenization, POS tagging, Named Entity Recognition, and relation extraction.",
    "about_model": "The major step involves the construction of the knowledge graph.",
    "image_url": null,
    "tags": [
      "Named Entity Recognition"
    ],
    "likes_count": 0,
    "downloads_count": 0,
    "views_count": 41,
    "source_org": "DEFFO TECH PRIVATE LIMITED",
    "license": "Other",
    "hosted_by": "Deffo Tech",
    "model_type": "Token Classification Model",
    "model_format": "Scikit-Learn",
    "visibility": "Restricted",
    "source_organization": "DEFFO TECH PRIVATE LIMITED",
    "sector": "Other",
    "updated_at": "2025-07-31T15:22:11+05:30",
    "created_by": "Sajith Ram K",
    "size": "845.80 KB"
  },
  {
    "id": 3,
    "title": "Parts of Speech Tagging and Named Entity Recognition",
    "description": "Parts of Speech Tagging and Named Entity Recognition",
    "about_model": "Model focuses on developing the Parts of Speech Tagging and Named Entity Recognition library from scratch.",
    "image_url": null,
    "tags": [
      "Named Entity Recognition"
    ],
    "likes_count": 0,
    "downloads_count": 0,
    "views_count": 26,
    "source_org": "DEFFO TECH PRIVATE LIMITED",
    "license": "Other",
    "hosted_by": "Deffo Tech",
    "model_type": "Named Entity Recognition (NER) Model",
    "model_format": "Scikit-Learn",
    "visibility": "Restricted",
    "source_organization": "DEFFO TECH PRIVATE LIMITED",
    "sector": "Other",
    "updated_at": "2025-07-31T15:21:27+05:30",
    "created_by": "Sajith Ram K",
    "size": "17.34 KB"
  },
  {
    "id": 4,
    "title": "Poem Generation",
    "description": "Transformer model is fine-tuned to build LLM from scratch for poem generation task.",
    "about_model": "Poem Generation includes the generation of nearly 10,000 training samples in JSON format including instruction, input and output.",
    "image_url": null,
    "tags": [
      "Transformer"
    ],
    "likes_count": 0,
    "downloads_count": 0,
    "views_count": 45,
    "source_org": "DEFFO TECH PRIVATE LIMITED",
    "license": "Other",
    "hosted_by": "Deffo Tech",
    "model_type": "Fine-Tuned Model",
    "model_format": "Transformers",
    "visibility": "Restricted",
    "source_organization": "DEFFO TECH PRIVATE LIMITED",
    "sector": "Other",
    "updated_at": "2025-07-31T15:20:58+05:30",
    "created_by": "Sajith Ram K",
    "size": "5.77 MB"
  },
  {
    "id": 5,
    "title": "Story Generation",
    "description": "Story Generation task is done from scratch.",
    "about_model": "Developed a base model for story generation.",
    "image_url": null,
    "tags": [
      "Transformer"
    ],
    "likes_count": 0,
    "downloads_count": 0,
    "views_count": 73,
    "source_org": "DEFFO TECH PRIVATE LIMITED",
    "license": "Other",
    "hosted_by": "Deffo Tech",
    "model_type": "Transformers",
    "model_format": "Transformers",
    "visibility": "Restricted",
    "source_organization": "DEFFO TECH PRIVATE LIMITED",
    "sector": "Other",
    "updated_at": "2025-07-29T18:21:23+05:30",
    "created_by": "Sajith Ram K",
    "size": "3.79 MB"
  },
  {
    "id": 6,
    "title": "Tamil Temple QA Model",
    "description": "Question and Answer generation of temples.",
    "about_model": "Model includes the Question and Answer pairs for Temple dataset.",
    "image_url": null,
    "tags": [
      "question-answering"
    ],
    "likes_count": 0,
    "downloads_count": 0,
    "views_count": 29,
    "source_org": "DEFFO TECH PRIVATE LIMITED",
    "license": "Other",
    "hosted_by": "Deffo Tech",
    "model_type": "Token Classification Model",
    "model_format": "Scikit-Learn",
    "visibility": "Restricted",
    "source_organization": "DEFFO TECH PRIVATE LIMITED",
    "sector": "Other",
    "updated_at": "2025-07-31T15:21:47+05:30",
    "created_by": "Sajith Ram K",
    "size": "349.39 KB"
  },
  {
    "id": 7,
    "title": "Text Generation",
    "description": "Text Generation - Base Model",
    "about_model": "We have constructed a base model for Tamil Text Generation.",
    "image_url": null,
    "tags": [
      "Text Generation"
    ],
    "likes_count": 0,
    "downloads_count": 0,
    "views_count": 51,
    "source_org": "DEFFO TECH PRIVATE LIMITED",
    "license": "Other",
    "hosted_by": "Deffo Tech",
    "model_type": "Transformers",
    "model_format": "Transformers",
    "visibility": "Restricted",
    "source_organization": "DEFFO TECH PRIVATE LIMITED",
    "sector": "Other",
    "updated_at": "2025-07-30T11:44:23+05:30",
    "created_by": "Sajith Ram K",
    "size": "4.54 MB"
  },
  {
    "id": 8,
    "title": "Project MAARG: AI-Powered Road Safety for India",
    "description": "An AI model that predicts real-time road accident risk by analyzing live traffic, weather, and historical data to enhance public safety.",
    "about_model": "This predictive model fuses data from traffic cameras, weather feeds, and accident records to generate dynamic risk heatmaps, enabling proactive traffic management and accident prevention.",
    "image_url": null,
    "tags": [
      "Smart Mobility",
      "Citizen Engagement",
      "Traffic Safety",
      "AI monitoring"
    ],
    "likes_count": 1,
    "downloads_count": 11,
    "views_count": 128,
    "source_org": "Thore Network PVT LTD",
    "license": "Attribution 4.0 International (CC BY-4.0)",
    "hosted_by": "Thore Network PVT LTD",
    "model_type": "Classification Model",
    "model_format": "ONNX",
    "visibility": "Open",
    "source_organization": "Thore Network PVT LTD",
    "sector": "Transportation, Logistics and Mobility",
    "updated_at": "2025-09-01T12:27:49+05:30",
    "created_by": "Alok Kumar",
    "size": "7.99 KB"
  },
  {
    "id": 9,
    "title": "Baaz-v1",
    "description": "Hindi text-to-image generation model using the Generative Adversarial Network.",
    "about_model": "Baaz is a Hindi T2I generation model using the Generative Adversarial Network (GAN). The model is specifically trained on region-specific data, including a Hindi language dataset, which has been prepared, pre-processed, and fed to the model. The corpus comprises 11,788 bird images from 200 species. Baaz is the first model of its kind developed for Hindi text-to-image generation and can create high-quality images of birds that accurately reflect the semantic content of Hindi text. The experimental results have been published in scientific reports, demonstrating that the Baaz model performs well and produces realistic images.\n\nSteps to Run the Model:\n1. Clone or download the repository:\n   git clone https://github.com/srinivasmudhiraj/-Hindi-T2I-generation-\n   cd -Hindi-T2I-generation-\n2. Install required dependencies:\n   pip install -r requirements.txt\n3. Navigate to the Notebook directory:\n   cd Hindi-T2I-generation-/Notebook/\n4. Download and place all the data, encoder, and model files.\n5. Navigate to the codes directory and run the Hindi T2I notebook for testing.",
    "image_url": null,
    "tags": [
      "Image generator",
      "natural language processing (NLP)",
      "Hindi",
      "Generative AI"
    ],
    "likes_count": 1,
    "downloads_count": 7,
    "views_count": 246,
    "source_org": "Central University of Punjab",
    "license": "MIT",
    "hosted_by": "Nakkala Srinivas Mudiraj and Satwinder Singh",
    "model_type": "GAN (Generative Adversarial Network) Model",
    "model_format": "PyTorch",
    "visibility": "Open",
    "source_organization": "Central University of Punjab",
    "sector": "Science, Technology and Research",
    "updated_at": "2025-10-28T17:16:29+05:30",
    "created_by": "Nakkala Srinivas Mudiraj",
    "size": "885.63 MB"
  },
  {
    "id": 10,
    "title": "BharatGen - Param 1: Indic-Scale Bilingual Foundation Model",
    "description": "Param 1 is a 2.9 billion parameter language model pretrained on English and Hindi, designed for text completion.",
    "about_model": "Param 1 is a 2.9-billion parameter foundation model developed for English and Hindi, capable of text generation and completion. Pretrained on high-quality, culturally rich datasets from diverse Indian domains comprising approximately 5 trillion tokens combined for English and Hindi, it delivers strong performance on bilingual tasks while maintaining computational efficiency. The model outperforms several models of similar size and task scope on standard benchmarks. Param 1 is developed by BharatGen: a suite of Generative AI technologies for India. For queries, please visit https://bharatgen.discourse.group/invites/BcouFsKk4g",
    "image_url": null,
    "tags": [
      "Large Language Model"
    ],
    "likes_count": 3,
    "downloads_count": 467,
    "views_count": 10820,
    "source_org": "BharatGen",
    "license": "MIT",
    "hosted_by": "Kundeshwar Pundalik, Piyush Sawarkar, Vedant Goswami, Ajay Nagpal, Smita Gautam, Bhagwan Pandit, Adugani Vanjari, Akanksh, Pankaj Singh, Rishi Bal, Prof. Rohit Saluja, Prof. Ganesh Ramakrishnan",
    "model_type": "Text Generation",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "BharatGen",
    "sector": "Science, Technology and Research",
    "updated_at": "2025-07-18T14:23:23+05:30",
    "created_by": "Kundeshwar Vijay Pundalik",
    "size": "8.44 GB"
  },
  {
    "id": 11,
    "title": "Fathom-R1-14B",
    "description": "Reasoning model that achieves top performance on IITJEE-25 math questions, surpassing several larger models with just 14B parameters and under 16K context.",
    "about_model": "Fathom-R1-14B is a 14-billion-parameter reasoning language model derived from DeepSeek-R1-Distilled-Qwen-14B. It is post-trained using supervised fine-tuning and model merging to achieve state-of-the-art mathematical reasoning performance within a 16K context window at a total training cost of just $499. The model attains strong results on olympiad-level benchmarks such as AIME-25, AIME-2025, and HMMT-25, rivaling or surpassing closed-source models while remaining fully open-sourced. Model weights, training datasets, and the post-training recipe are publicly available.",
    "image_url": null,
    "tags": [
      "LLMs",
      "Reasoning",
      "Complex Reasoning",
      "Code Generation",
      "LRM",
      "Fractal",
      "FractalAIResearch",
      "Problem Solving",
      "Complex Business Problem Solving"
    ],
    "likes_count": 1,
    "downloads_count": 49,
    "views_count": 338,
    "source_org": "Fractal AI Research",
    "license": "MIT",
    "hosted_by": "FractalAIResearch",
    "model_type": "Large Language Models",
    "model_format": "PyTorch",
    "visibility": "Open",
    "source_organization": "Fractal AI Research",
    "sector": "Science, Technology and Research",
    "updated_at": "2025-06-09T14:32:58+05:30",
    "created_by": "Surya Tiwari",
    "size": "N.A."
  },
  {
    "id": 12,
    "title": "Ganga-2-1B",
    "description": "The first pre-trained Hindi model by any academic research lab in India.",
    "about_model": "Project Unity is an initiative to address India's linguistic diversity and richness by creating a comprehensive resource covering the country's major languages. We strive to achieve state-of-the-art performance in understanding and generating text in Indian languages. To achieve this, we train models on monolingual regional languages of India. Our first release is the Ganga-1B model, which has been trained on a large dataset of public domain web-crawled Hindi language data, including news articles, web documents, books, government publications, educational materials, and social media conversations (filtered for quality). Additionally, the dataset has been further curated by native Indian speakers to ensure high quality.\n\nSignificantly, the Ganga-2-1B model outperforms existing open-source models that support Indian languages, even at sizes of up to 7 billion parameters.\n\nThe instruct-tuned model Ganga-2-1B is trained on a monolingual Hindi language dataset as part of Project Unity. We propose the name Ganga to honor the longest river flowing through the Hindi-speaking region of India.\n\nDisclaimer: This is a text-completion model designed for fine-tuning on downstream tasks. It is not intended for direct use as a chat or instruction-following model.\n\nRecommendations ‼️\nThis model described is a research preview and is under ongoing iterative updations, and as such, it only provides limited safety measures. Additionally, it may generate offensive content. It is strictly prohibited to use the model for any illegal, harmful, violent, racist, or sexual purposes.",
    "image_url": null,
    "tags": [
      "Text Generation"
    ],
    "likes_count": 1,
    "downloads_count": 13,
    "views_count": 169,
    "source_org": "IITGN",
    "license": "Apache 2.0",
    "hosted_by": "Aamod Thakur, Mayank Singh",
    "model_type": "Large Language Models",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "IITGN",
    "sector": "Science, Technology and Research",
    "updated_at": "2025-08-20T11:13:32+05:30",
    "created_by": "Lingo Research Group",
    "size": "1.88 GB"
  },
  {
    "id": 13,
    "title": "sarvam-1",
    "description": "India's first Indic model, pretrained on 4 trillion tokens.",
    "about_model": "Sarvam-1 is a 2-billion parameter language model designed for Indian languages, optimized for token efficiency and high-quality training data. It outperforms larger models in several benchmarks, providing superior performance on Indic language tasks with enhanced computational efficiency. Built with a custom tokenizer and trained on 4 trillion tokens from diverse sources, Sarvam-1 is ideal for applications like translation and edge device deployment.",
    "image_url": null,
    "tags": [],
    "likes_count": 1,
    "downloads_count": 111,
    "views_count": 1849,
    "source_org": "Sarvam AI",
    "license": "Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)",
    "hosted_by": "sarvamai",
    "model_type": "Multilingual Language Model",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Sarvam AI",
    "sector": "Science, Technology and Research",
    "updated_at": "2025-02-24T13:15:49+05:30",
    "created_by": "Aashay Sachdeva",
    "size": "N.A."
  },
  {
    "id": 14,
    "title": "shuka-v1",
    "description": "Multilingual audio to text model.",
    "about_model": "Shuka v1 is an innovative audio understanding model for Indic languages, combining Saaras v1 encoder and Meta's Llama3-8B-Instruct as the decoder. Trained on less than 100 hours of data, it outperforms larger models in audio-based question-answering tasks and supports fine-tuning for customized use cases. Shuka v1 is available open-source, marking the start of advancements in audio language models for Indic languages.",
    "image_url": null,
    "tags": [
      "audio-llms"
    ],
    "likes_count": 1,
    "downloads_count": 50,
    "views_count": 585,
    "source_org": "Sarvam AI",
    "license": "CC0 1.0 Public Domain",
    "hosted_by": "sarvamai",
    "model_type": "Audio-to-text",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Sarvam AI",
    "sector": "Science, Technology and Research",
    "updated_at": "2025-02-24T13:15:11+05:30",
    "created_by": "Aashay Sachdeva",
    "size": "N.A."
  },
  {
    "id": 15,
    "title": "Electrical Grid Distribution Model",
    "description": "This predictive model can map medium-voltage electrical distribution infrastructure in any country.",
    "about_model": "The model functions by applying custom algorithms to publicly available datasets, including NASA satellite imagery, land cover datasets from MODIS, and roadway locations from OpenStreetMap. Meta has shared its code and a tutorial on how the model is constructed, allowing anyone to replicate it for new countries. The facebookresearch/many-to-many-dijkstra project is licensed under the MIT License, a short and permissive license that requires preservation of copyright and license notices while allowing licensed works, modifications, and larger works to be distributed under different terms and without source code.",
    "image_url": null,
    "tags": [
      "electricity",
      "utilities",
      "power distribution",
      "energy data",
      "Power and Energy"
    ],
    "likes_count": 0,
    "downloads_count": 2,
    "views_count": 45,
    "source_org": "FACEBOOK INDIA ONLINE SERVICES PRIVATE LIMITED",
    "license": "MIT",
    "hosted_by": "facebookresearch",
    "model_type": "Autonomous Navigation Model",
    "model_format": "Other",
    "visibility": "Open",
    "source_organization": "FACEBOOK INDIA ONLINE SERVICES PRIVATE LIMITED",
    "sector": "Housing, Urban Planning and Infrastructure",
    "updated_at": "2025-11-05T11:13:30+05:30",
    "created_by": "Sudhamay Maity",
    "size": "N.A."
  },
  {
    "id": 16,
    "title": "BiomedBERT - Domain-Specific Biomedical Language Model",
    "description": "A biomedical NLP model pre-trained from scratch on abstracts and full-text articles from PubMed and PubMed Central, achieving state-of-the-art performance on biomedical language understanding tasks.",
    "about_model": "BiomedBERT (formerly PubMedBERT (abstracts + full text)) is a domain-specific biomedical language model developed by Microsoft. Unlike general NLP models that start with broad-domain corpora, BiomedBERT is pre-trained from scratch using PubMed abstracts and full-text articles from PubMed Central, enabling superior performance on biomedical NLP tasks. Key capabilities of BiomedBERT include biomedical text classification, named entity recognition (NER) for medical terminology, question answering in the medical domain, and biomedical language inference and reasoning. This model outperforms general-domain language models on various benchmarks and currently holds the top score on the Biomedical Language Understanding and Reasoning Benchmark. BiomedBERT is intended for research purposes only and should not be used for clinical decision-making. It serves as a valuable tool for biomedical AI researchers, medical text mining, and healthcare-related NLP applications.",
    "image_url": null,
    "tags": [
      "Fill-Mask",
      "Transformers",
      "PyTorch",
      "JAX",
      "English",
      "Bert",
      "expert",
      "inference endpoints"
    ],
    "likes_count": 0,
    "downloads_count": 45,
    "views_count": 524,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "MIT",
    "hosted_by": "Microsoft",
    "model_type": "Fill-Mask",
    "model_format": "N.A.",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-04-11T11:47:29+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 17,
    "title": "BiomedCLIP-PubMedBERT_256-vit_base_patch16_224: A biomedical vision-language foundation model",
    "description": "A biomedical vision-language foundation model trained on PMC-15M using PubMedBERT as the text encoder and Vision Transformer as the image encoder, optimized for cross-modal retrieval, classification, and visual question answering in medical AI applications.",
    "about_model": "BiomedCLIP is a state-of-the-art biomedical vision-language model designed for multimodal learning in medical AI. Developed by Microsoft, it is pre-trained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central. The model combines PubMedBERT as the text encoder for domain-specific language understanding and a Vision Transformer (ViT) as the image encoder with specialized adaptations for medical imaging tasks. BiomedCLIP significantly outperforms prior vision-language models in various medical AI benchmarks and supports applications such as cross-modal retrieval (text-to-image and image-to-text search), zero-shot image classification for medical images, and visual question answering (VQA) in radiology and pathology. Trained on diverse medical imaging modalities including radiography, microscopy, and histology, BiomedCLIP establishes new performance standards in biomedical vision-language tasks. The model is intended for research purposes only and is not suitable for clinical decision-making or commercial deployment.",
    "image_url": null,
    "tags": [
      "Zero-Shot Image Classification",
      "OpenCLIP",
      "English",
      "clip",
      "biology",
      "medical"
    ],
    "likes_count": 0,
    "downloads_count": 16,
    "views_count": 302,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "MIT",
    "hosted_by": "Microsoft",
    "model_type": "Zero-Shot Image Classification",
    "model_format": "N.A.",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-04-11T11:55:10+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 18,
    "title": "BiomedVLP-CXR-BERT - Domain-Specific BERT for Chest X-ray Reports",
    "description": "A specialized biomedical language model pre-trained on radiology reports from MIMIC-CXR and PubMed, optimized for radiology NLP tasks such as masked language modeling, phrase grounding, and medical text classification.",
    "about_model": "CXR-BERT is a domain-specific BERT-based language model designed for chest X-ray (CXR) report analysis. Developed by Microsoft, it improves performance in radiology natural language inference (RadNLI), masked token prediction, and image-text classification by leveraging an enhanced vocabulary and novel pretraining methods.\n\nCXR-BERT is trained in multiple stages:\n1. CXR-BERT-general: Pre-trained using Masked Language Modeling (MLM) on PubMed abstracts, MIMIC-III, and MIMIC-CXR clinical notes, making it applicable to broader biomedical NLP tasks.\n2. CXR-BERT-specialized: Further pre-trained on CXR-specific data to refine its understanding of radiology terminology and integrate vision-language embeddings.\n\nKey Capabilities:\n1. Zero-shot phrase grounding for radiology text interpretation.\n2. Improved medical text encoding for radiology NLP applications.\n3. Enhanced performance over models like PubMedBERT and ClinicalBERT in medical text classification and inference tasks.\n\nCXR-BERT is intended for research purposes only and is not suitable for clinical diagnosis. It serves as a foundational model for AI researchers working on radiology NLP, medical report generation, and automated healthcare documentation.\n\nThe resulting model demonstrates improved performance on radiology natural language inference, radiology masked language model token prediction, and downstream vision-language processing tasks such as zero-shot phrase grounding and image classification. First, CXR-BERT-general is pretrained from a randomly initialized BERT model via Masked Language Modeling (MLM) on PubMed abstracts and clinical notes from the publicly available MIMIC-III and MIMIC-CXR datasets. In this regard, the general model is expected to be applicable for research in clinical domains beyond chest radiology through domain-specific fine-tuning. CXR-BERT-specialized is continually pretrained from CXR-BERT-general to further specialize in the chest X-ray domain. At the final stage, CXR-BERT is trained in a multimodal contrastive learning framework similar to CLIP, where the latent representation of the [CLS] token is used to align text and image embeddings.",
    "image_url": null,
    "tags": [
      "Fill-Mask",
      "Transformers",
      "PyTorch",
      "JAX",
      "English",
      "Bert",
      "expert",
      "inference endpoints",
      "arxiv:2204.09817",
      "arxiv:2103.00020"
    ],
    "likes_count": 0,
    "downloads_count": 7,
    "views_count": 102,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "MIT",
    "hosted_by": "Microsoft",
    "model_type": "Fill-Mask",
    "model_format": "N.A.",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-04-11T11:56:44+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 19,
    "title": "BiomedVLP-CXR-BERT-Specialized - Domain-Specific Vision-Language Model for Radiology",
    "description": "A specialized biomedical vision-language model trained on chest X-ray reports from MIMIC-CXR and PubMed, optimized for radiology NLP, masked language modeling, and multi-modal contrastive learning with image-text alignment.",
    "about_model": "CXR-BERT-Specialized is an advanced BERT-based model designed for chest X-ray (CXR) report analysis. Developed by Microsoft, it builds upon CXR-BERT-General and further refines its understanding of radiology texts while enhancing vision-language processing through multi-modal contrastive learning with ResNet-50.\n\nThe model is trained in three stages:\n1. CXR-BERT-General: Pre-trained with Masked Language Modeling (MLM) on PubMed abstracts, MIMIC-III, and MIMIC-CXR to develop broad biomedical NLP capabilities.\n2. CXR-BERT-Specialized: Further pre-trained on chest X-ray reports to specialize in radiology language inference and masked token prediction.\n3. Multi-Modal Contrastive Learning: Trained alongside ResNet-50 on MIMIC-CXR images using SimCLR, aligning text and image embeddings for zero-shot phrase grounding and radiology report classification.\n\nKey Features include superior medical text understanding for radiology NLP tasks, multi-modal contrastive learning for image-text embedding alignment, and state-of-the-art performance in radiology natural language inference (RadNLI) and zero-shot phrase grounding on MS-CXR benchmarks. Performance highlights include RadNLI accuracy of 65.21%, mask prediction accuracy of 81.58%, and best-in-class CNR score of 1.142 on MS-CXR.\n\nCXR-BERT-Specialized is intended for research purposes only and is not suitable for clinical diagnosis. It serves as a powerful tool for AI researchers working on radiology NLP, medical image-text analysis, and automated healthcare documentation.",
    "image_url": null,
    "tags": [
      "Fill-Mask",
      "Transformers",
      "PyTorch",
      "JAX",
      "English",
      "cxr-bert",
      "expert",
      "inference endpoints",
      "Feature Extraction",
      "custom_code",
      "arxiv:2204.09817",
      "arxiv:2103.00020",
      "arxiv:2002.05709"
    ],
    "likes_count": 0,
    "downloads_count": 5,
    "views_count": 113,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "MIT",
    "hosted_by": "Microsoft",
    "model_type": "Fill-Mask",
    "model_format": "N.A.",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-04-11T11:58:05+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 20,
    "title": "BioViL-T - Temporal Vision-Language Model for Radiology",
    "description": "A domain-specific vision-language model designed for analyzing chest X-rays and radiology reports, leveraging temporal multi-modal pre-training for improved biomedical inference, phrase grounding, and image-text alignment.",
    "about_model": "BioViL-T is a vision-language model developed by Microsoft for biomedical AI applications, particularly radiology analysis. It extends its predecessor BioViL by incorporating temporal multi-modal pre-training, which captures the temporal structure of medical data and leads to enhanced downstream performance in radiology-related tasks.\n\nKey features of BioViL-T include: (1) Joint image-text learning for improved radiology phrase grounding and inference. (2) Image-text classification using Vision Transformer (ViT) and ResNet-50 as hybrid encoders. (3) Improved sentence embeddings for radiology natural language inference (RadNLI) benchmark. (4) Pre-training on MIMIC-CXR and PubMed clinical notes for robust biomedical text representation.\n\nCompared to other radiology NLP models such as PubMedBERT and CXR-BERT, BioViL-T demonstrates superior performance in both static and temporal representation tasks, including zero-shot phrase grounding and medical image-text alignment. This model is intended for research purposes only and is not suitable for clinical diagnosis or commercial deployment. It serves as a powerful tool for AI researchers working on radiology NLP, medical imaging analysis, and multi-modal biomedical AI.",
    "image_url": null,
    "tags": [
      "Feature Extraction",
      "Transformers",
      "PyTorch",
      "safetensors",
      "English",
      "expert",
      "custom_code",
      "arxiv:2301.04558",
      "arxiv:2204.09817"
    ],
    "likes_count": 0,
    "downloads_count": 10,
    "views_count": 244,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "MIT",
    "hosted_by": "Microsoft",
    "model_type": "Feature Extraction",
    "model_format": "N.A.",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-04-11T11:52:59+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 21,
    "title": "MAIRA-2 - Multimodal AI for Radiology Report Generation",
    "description": "A multimodal Transformer model designed for generating grounded and non-grounded radiology reports from chest X-rays, integrating vision and language understanding for medical AI research.",
    "about_model": "MAIRA-2 is a multimodal Transformer model developed by Microsoft Research Health Futures for automated radiology report generation. It processes chest X-ray images and generates structured reports, with or without grounding, by linking findings to specific image regions. The model is built using the RAD-DINO-MAIRA-2 image encoder and the Vicuna-7B-V1.5 language model, enabling advanced medical text generation.\n\nMAIRA-2 is intended for research purposes only and is not suitable for clinical practice due to potential biases and limitations in generalizability. Trained on datasets such as MIMIC-CXR, PadChest, and USMix, it aims to facilitate comparative studies in AI-driven radiology analysis. The model supports findings generation, phrase grounding, and structured medical text generation, providing valuable insights into automated medical imaging interpretation while promoting fairness and responsible AI use in healthcare.",
    "image_url": null,
    "tags": [
      "Text Generation",
      "Transformers",
      "safetensors",
      "maira2",
      "conversational",
      "custom_code"
    ],
    "likes_count": 0,
    "downloads_count": 15,
    "views_count": 210,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "Other",
    "hosted_by": "Microsoft Health Futures",
    "model_type": "Text Generation",
    "model_format": "PyTorch",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-08-20T11:17:02+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 22,
    "title": "medgemma-27b-it",
    "description": "An instruction-tuned multimodal large language model from Google designed for medical text and image understanding, optimized for healthcare-focused reasoning, image-text comprehension, and medical report generation.",
    "about_model": "MedGemma is a collection of Gemma 3 variants trained for high performance on medical text and image comprehension tasks. The medgemma-27b-it model is an instruction-tuned, multimodal variant designed to accelerate the development of healthcare-based AI applications. It integrates a SigLIP image encoder that has been specifically pre-trained on medical imagery such as chest X-rays, dermatology images, histopathology slides, ophthalmology images, and fundus images.\n\nThe language model component is trained on a diverse set of medical data, including medical text, medical question-answer pairs, electronic health records, radiology images, pathology patches, and dermatology images. MedGemma supports advanced tasks such as medical text generation, image-text reasoning, zero-shot image classification, radiology report generation, and medical embeddings.\n\nThe 27B variant is optimized for strong medical reasoning and multimodal understanding, making it suitable for use cases that require both medical text and medical image analysis. It is available only in instruction-tuned form and has been evaluated on a wide range of clinically relevant benchmarks. MedGemma is intended for research and development purposes in healthcare AI and is not approved for direct clinical use.",
    "image_url": null,
    "tags": [
      "Transformers",
      "safetensors",
      "image-text-to-text",
      "text-generation-inference",
      "conversational",
      "medical",
      "Image Feature Extraction",
      "Zero-Shot Image Classification",
      "image classification",
      "chest-x-ray",
      "pathology",
      "radiology report generation",
      "medical-embeddings",
      "gemma3"
    ],
    "likes_count": 0,
    "downloads_count": 5,
    "views_count": 107,
    "source_org": "Google LLC",
    "license": "Other",
    "hosted_by": "Google",
    "model_type": "Large Language Models",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Google LLC",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-09-18T15:57:25+05:30",
    "created_by": "Amrita Kamat",
    "size": "N.A."
  },
  {
    "id": 23,
    "title": "medgemma-27b-text-it",
    "description": "An instruction-tuned, text-only large language model optimized for medical text understanding, clinical reasoning, and healthcare-focused text generation.",
    "about_model": "MedGemma is a collection of Gemma 3 variants trained for high performance on medical text and image comprehension. The medgemma-27b-text-it model is a 27B parameter, text-only, instruction-tuned variant designed to accelerate the development of healthcare-based AI applications.\n\nMedGemma 27B has been trained exclusively on medical text and optimized for inference-time computation. It is available only as an instruction-tuned model and is intended for tasks such as medical text generation, clinical reasoning, conversational healthcare assistants, and medical question answering.\n\nMedGemma variants have been evaluated on a range of clinically relevant benchmarks to illustrate baseline performance, using both open benchmark datasets and curated medical datasets. Developers can further fine-tune MedGemma models for improved task-specific performance. A full technical report is expected to be released separately.",
    "image_url": null,
    "tags": [
      "Transformers",
      "safetensors",
      "gemma3_text",
      "Text Generation",
      "medical",
      "clinical-reasoning",
      "thinking",
      "conversational",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "likes_count": 0,
    "downloads_count": 0,
    "views_count": 25,
    "source_org": "Google LLC",
    "license": "Other",
    "hosted_by": "Google",
    "model_type": "Large Language Models",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Google LLC",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-09-18T15:57:48+05:30",
    "created_by": "Amrita Kamat",
    "size": "N.A."
  },
  {
    "id": 24,
    "title": "MedGemma-4b-it",
    "description": "An instruction-tuned multimodal large language model optimized for medical text and image understanding, designed for healthcare-focused reasoning, image-text comprehension, and clinical AI applications.",
    "about_model": "MedGemma is a collection of Gemma 3 variants trained for strong performance on medical text and image comprehension tasks. MedGemma-4B-it is a multimodal, instruction-tuned variant that integrates a SigLIP image encoder pre-trained on diverse medical imaging data, including chest X-rays, dermatology images, histopathology slides, and ophthalmology images.\n\nThe language model component is trained on a wide range of medical data such as medical text, medical question-answer pairs, electronic health records, radiology images, pathology patches, and dermatology images. MedGemma-4B-it supports tasks including medical text generation, image-text reasoning, zero-shot image classification, radiology report generation, and medical embeddings.\n\nMedGemma-4B is available in both pre-trained and instruction-tuned versions, with the instruction-tuned variant serving as a strong starting point for most healthcare applications. The model has been evaluated on multiple clinically relevant benchmarks using both open and curated datasets. MedGemma is intended for research and development in medical AI and is not approved for direct clinical use.",
    "image_url": null,
    "tags": [
      "Transformers",
      "safetensors",
      "endpoints_compatible",
      "image-text-to-text",
      "text-generation-inference",
      "conversational",
      "gemma3",
      "radiology",
      "medical",
      "regions:us",
      "clinical-reasoning",
      "dermatology",
      "chest-x-ray",
      "pathology",
      "ophthalmology"
    ],
    "likes_count": 0,
    "downloads_count": 1,
    "views_count": 43,
    "source_org": "Google LLC",
    "license": "Other",
    "hosted_by": "Google",
    "model_type": "Large Language Models",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Google LLC",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-09-18T15:58:12+05:30",
    "created_by": "Amrita Kamat",
    "size": "N.A."
  },
  {
    "id": 25,
    "title": "parrotlet-a-en-5b",
    "description": "A purpose-driven automatic speech recognition (ASR) model for English medical speech, optimized for transcription in Indian healthcare settings.",
    "about_model": "Parrotlet-a-en-5b is a purpose-built automatic speech recognition (ASR) model specifically trained for English speech in Indian healthcare environments. The model is optimized for transcribing medical speech and clinical conversations with high accuracy. It combines a Whisper V3 large encoder with a MedGemma-4B decoder through a lightweight projector layer, enabling efficient and robust speech-to-text conversion.\n\nThe model is designed to support medical ASR use cases such as clinical note dictation, doctor–patient interaction transcription, and healthcare documentation workflows. A detailed technical overview and release information are available through Eka Care’s official blog, highlighting the model’s design choices and healthcare-focused optimizations.",
    "image_url": null,
    "tags": [
      "Transformers",
      "safetensors",
      "speech LLM",
      "Feature Extraction",
      "medical",
      "speech",
      "ASR",
      "Automatic Speech Recognition",
      "custom_code",
      "en",
      "region:us"
    ],
    "likes_count": 0,
    "downloads_count": 5,
    "views_count": 58,
    "source_org": "Eka Care",
    "license": "MIT",
    "hosted_by": "ekacare",
    "model_type": "Transformers",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Eka Care",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-07-31T15:23:00+05:30",
    "created_by": "Sankalp Gulati",
    "size": "N.A."
  },
  {
    "id": 26,
    "title": "parrotlet e",
    "description": "A state-of-the-art multilingual medical entity embedding model designed for understanding and linking medical terms across Indian languages.",
    "about_model": "Parrotlet-e is a state-of-the-art multilingual medical embedding model designed for understanding and linking medical terms across Indian languages. It is optimized for entity-level representation of clinical concepts such as symptoms, diagnoses, and anatomical structures, enabling accurate medical coding, semantic search, and cross-lingual retrieval in healthcare applications.\n\nThe model is fine-tuned from bge-m3 using weakly supervised contrastive learning with Multi-Similarity Loss on over 18 million multilingual medical term pairs aligned with SNOMED CT and UMLS. It supports both native and romanized scripts across 12 Indic languages and English, and is robust to abbreviations, spelling variations, and colloquial expressions commonly found in clinical documentation.\n\nIndic languages supported include Hindi, Kannada, Marathi, Malayalam, Tamil, Telugu, Odia, Assamese, Bengali, Urdu, Gujarati, and Punjabi. Parrotlet-e is intended for research and production use cases such as medical entity linking, semantic search, multilingual clinical NLP, and healthcare information retrieval.",
    "image_url": null,
    "tags": [
      "Multilingual AI",
      "medical-embeddings",
      "base_model:BAAI/bge-m3",
      "base_model:finetune:BAAI/bge-m3"
    ],
    "likes_count": 0,
    "downloads_count": 3,
    "views_count": 42,
    "source_org": "Eka Care",
    "license": "Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)",
    "hosted_by": "ekacare",
    "model_type": "Multilingual Model",
    "model_format": "PyTorch",
    "visibility": "Open",
    "source_organization": "Eka Care",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-11-13T11:42:02+05:30",
    "created_by": "Sankalp Gulati",
    "size": "N.A."
  },
  {
    "id": 27,
    "title": "parrotlet-v-lite-4b",
    "description": "Purpose-driven medical records parsing model for the Indian healthcare context.",
    "about_model": "Parrotlet-v-lite-4b is a purpose-built vision LLM designed specifically for parsing medical records in the Indian healthcare ecosystem. The model is optimized for key downstream tasks including lab report parsing, digital prescription parsing, document classification, and pill extraction.\n\nIt is trained to handle real-world clinical documents and supports structured information extraction from diverse medical record formats. The model is suitable for automating healthcare workflows such as clinical data digitization, record categorization, and medical document understanding.\n\nA detailed technical description of the model and its design choices is available via Eka Care’s official blog post.",
    "image_url": null,
    "tags": [
      "Transformers",
      "Feature Extraction",
      "medical record parsing",
      "lab report parsing",
      "prescription parsing",
      "Image-to-Text",
      "base_model:google/medgemma-4b-it"
    ],
    "likes_count": 0,
    "downloads_count": 3,
    "views_count": 48,
    "source_org": "Eka Care",
    "license": "MIT",
    "hosted_by": "ekacare",
    "model_type": "Transformers",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Eka Care",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-07-31T15:23:32+05:30",
    "created_by": "Sankalp Gulati",
    "size": "N.A."
  },
  {
    "id": 28,
    "title": "RAD-DINO-MAIRA-2",
    "description": "Vision Transformer model for medical imaging, trained for chest X-ray encoding using self-supervised learning.",
    "about_model": "RAD-DINO-MAIRA-2 is a Vision Transformer (ViT) model designed for encoding chest X-ray images using self-supervised learning based on the DINOv2 framework. It is an enhanced version of RAD-DINO, trained on a significantly larger dataset to improve performance in medical imaging tasks.\n\nDeveloped by Microsoft Health Futures, the model serves as the vision backbone for MAIRA-2, enabling advanced radiology report generation by extracting meaningful visual features from medical images. It supports a wide range of downstream tasks, including image classification using classifiers trained on extracted embeddings, image segmentation with decoder-trained patch tokens, medical image retrieval via nearest-neighbor search, clustering using learned image embeddings, and integration with language models for radiology report generation.\n\nRAD-DINO-MAIRA-2 is trained on approximately 1.4 million chest X-ray images from datasets such as MIMIC-CXR, NIH-CXR, PadChest, and CheXpert. While the model provides robust representations of medical images and advances AI-powered medical imaging, it is intended strictly for research purposes and not for clinical use. Careful validation is required before any deployment in healthcare applications.",
    "image_url": null,
    "tags": [
      "Image Feature Extraction",
      "Transformers",
      "safetensors",
      "dinov2",
      "medical imaging",
      "chest x-ray"
    ],
    "likes_count": 0,
    "downloads_count": 11,
    "views_count": 180,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "Other",
    "hosted_by": "Microsoft Health Futures",
    "model_type": "Image Feature Extraction",
    "model_format": "PyTorch",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-08-20T11:57:00+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 29,
    "title": "RAD-DINO",
    "description": "Vision Transformer model for medical image encoding, trained with self-supervised learning for chest X-ray analysis.",
    "about_model": "RAD-DINO is a Vision Transformer (ViT) model developed by Microsoft Health Futures for encoding chest X-ray images using self-supervised learning based on the DINOv2 framework. It serves as a robust medical image feature extractor, enabling integration with downstream tasks such as image classification, segmentation, retrieval, and radiology report generation.\n\nThe model is trained on 882,775 identified chest X-ray images from large-scale datasets including MIMIC-CXR, NIH-CXR, PadChest, CheXpert, and BRAX. RAD-DINO supports multiple medical imaging applications, including image classification using classifiers trained on extracted features, image segmentation with decoder-trained patch tokens, clustering based on learned image embeddings, image retrieval via nearest-neighbor search, and medical report generation when paired with a language model.\n\nRAD-DINO was trained on Azure Machine Learning using 16 A100 GPUs, ensuring efficient large-scale processing. However, due to potential biases and dataset limitations, the model is intended strictly for research purposes and is not suitable for clinical use. It provides a strong foundation for advancing AI-driven medical imaging and radiology automation.",
    "image_url": null,
    "tags": [
      "Image Feature Extraction",
      "Transformers",
      "safetensors",
      "dinov2",
      "medical imaging",
      "chest x-ray"
    ],
    "likes_count": 0,
    "downloads_count": 10,
    "views_count": 157,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "Other",
    "hosted_by": "Microsoft Health Futures",
    "model_type": "Image Feature Extraction",
    "model_format": "PyTorch",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-08-20T11:17:30+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 30,
    "title": "RadEdit",
    "description": "A deep learning biomedical vision model for editing chest X-ray images to discover failure cases in medical AI systems.",
    "about_model": "RadEdit is a deep learning–based text-to-image latent diffusion model developed by Microsoft Health Futures for stress-testing biomedical vision models. It enables controlled editing of chest X-ray images by adding or removing abnormalities based on textual prompts, allowing researchers to evaluate robustness, failure cases, and generalization of medical AI systems.\n\nThe model is trained on 487,680 chest X-ray images from large-scale datasets including MIMIC-CXR, NIH-CXR, and CheXpert. RadEdit supports multiple use cases such as modifying X-rays using text prompts (e.g., adding pleural effusion or removing pneumothorax), generating synthetic X-rays conditioned on radiology reports, and assessing AI model robustness across diverse disease distributions.\n\nRadEdit integrates a DINO-based image encoder and an SDXL-VAE (autoencoder) to enable high-quality image editing. The model is intended strictly for research purposes and is not suitable for clinical use, as it may introduce biases or artifacts. It serves as a valuable tool for AI researchers studying dataset shifts, failure modes, and model reliability in medical imaging.",
    "image_url": null,
    "tags": [
      "Diffusers",
      "safetensors",
      "MIMIC-CXR",
      "NIH-CXR",
      "CheXpert",
      "medical imaging",
      "latent diffusion"
    ],
    "likes_count": 0,
    "downloads_count": 11,
    "views_count": 104,
    "source_org": "Microsoft Corporation (India) Pvt. Ltd.",
    "license": "Other",
    "hosted_by": "Microsoft Health Futures",
    "model_type": "Diffusers",
    "model_format": "N.A.",
    "visibility": "Open",
    "source_organization": "Microsoft Corporation (India) Pvt. Ltd.",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-04-11T20:06:45+05:30",
    "created_by": "Vikram Malhotra",
    "size": "N.A."
  },
  {
    "id": 31,
    "title": "txgemma-27b-chat",
    "description": "A conversational large language model fine-tuned for therapeutic development and drug discovery tasks.",
    "about_model": "TxGemma is a collection of lightweight, state-of-the-art open language models built on Gemma 2 and fine-tuned for therapeutic development. The txgemma-27b-chat variant is a conversational model designed to process and understand information related to therapeutic modalities and biological targets, including small molecules, proteins, nucleic acids, diseases, and cell lines.\n\nThe model excels at interactive tasks such as explaining predictions, reasoning through drug discovery workflows, and serving as a conversational agent for biomedical research. It is fine-tuned from Gemma 2 using a diverse set of instruction-tuning datasets curated from the Therapeutics Data Commons (TDC).\n\nCompared to the base prediction models, txgemma-27b-chat offers greater flexibility for multi-turn interactions and explanatory dialogue, though this comes with a trade-off in raw prediction performance. The model is intended for research and development use in therapeutic and drug discovery applications.",
    "image_url": null,
    "tags": [
      "Transformers",
      "safetensors",
      "gemma2",
      "Text Generation",
      "therapeutics",
      "drug-development",
      "conversational"
    ],
    "likes_count": 0,
    "downloads_count": 1,
    "views_count": 21,
    "source_org": "Google LLC",
    "license": "Other",
    "hosted_by": "Google",
    "model_type": "Large Language Models",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Google LLC",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-09-18T15:58:39+05:30",
    "created_by": "Amrita Kamat",
    "size": "N.A."
  },
  {
    "id": 32,
    "title": "txgemma-27b-predict",
    "description": "A prediction-focused large language model fine-tuned for therapeutic development and drug discovery.",
    "about_model": "TxGemma is a collection of lightweight, state-of-the-art open language models built upon Gemma 2 and fine-tuned for therapeutic development. The txgemma-27b-predict variant is optimized for prediction-centric tasks and expects a narrower prompting format compared to conversational variants.\n\nThe model is designed to process and understand information related to therapeutic modalities and biological targets, including small molecules, proteins, nucleic acids, diseases, and cell lines. It excels at property prediction and structured inference tasks commonly used in drug discovery workflows.\n\nTxGemma is fine-tuned from Gemma 2 using a diverse set of instruction-tuning datasets curated from the Therapeutics Data Commons (TDC). While less flexible for multi-turn dialogue than chat variants, txgemma-27b-predict delivers stronger raw prediction performance, making it suitable as a foundation model for downstream therapeutic and biomedical inference tasks.",
    "image_url": null,
    "tags": [
      "Transformers",
      "safetensors",
      "gemma2",
      "Text Generation",
      "therapeutics",
      "drug-development"
    ],
    "likes_count": 0,
    "downloads_count": 1,
    "views_count": 26,
    "source_org": "Google LLC",
    "license": "Other",
    "hosted_by": "Google",
    "model_type": "Large Language Models",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Google LLC",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-09-18T15:57:53+05:30",
    "created_by": "Amrita Kamat",
    "size": "N.A."
  },
  {
    "id": 33,
    "title": "txgemma-9b-chat",
    "description": "A conversational large language model fine-tuned for therapeutic development and drug discovery.",
    "about_model": "TxGemma is a collection of lightweight, state-of-the-art open language models built upon Gemma 2 and fine-tuned for therapeutic development. The txgemma-9b-chat variant is designed for interactive, conversational use and supports multi-turn dialogue.\n\nThe model is capable of processing and understanding information related to therapeutic modalities and biological targets, including small molecules, proteins, nucleic acids, diseases, and cell lines. TxGemma excels at tasks such as property prediction and can also act as a conversational agent for drug discovery workflows.\n\nFine-tuned from Gemma 2 using a diverse set of instruction-tuning datasets curated from the Therapeutics Data Commons (TDC), the chat variant provides greater flexibility in prompting and interaction compared to prediction-only variants. This conversational flexibility comes with a slight trade-off in raw prediction performance but enables richer explanations and reasoning in therapeutic and biomedical contexts.",
    "image_url": null,
    "tags": [
      "Transformers",
      "safetensors",
      "gemma2",
      "Text Generation",
      "therapeutics",
      "drug-development",
      "conversational"
    ],
    "likes_count": 0,
    "downloads_count": 4,
    "views_count": 38,
    "source_org": "Google LLC",
    "license": "Other",
    "hosted_by": "Google",
    "model_type": "Large Language Models",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Google LLC",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-09-18T15:56:27+05:30",
    "created_by": "Amrita Kamat",
    "size": "N.A."
  },
  {
    "id": 34,
    "title": "txgemma-9b-predict",
    "description": "A prediction-focused large language model fine-tuned for therapeutic development and drug discovery.",
    "about_model": "TxGemma is a collection of lightweight, state-of-the-art open language models built upon Gemma 2 and fine-tuned for therapeutic development. The txgemma-9b-predict variant is optimized for prediction tasks and expects a narrow form of prompting compared to conversational variants.\n\nThe model is designed to process and understand information related to therapeutic modalities and biological targets, including small molecules, proteins, nucleic acids, diseases, and cell lines. It excels at tasks such as property prediction and serves as a strong foundation for further fine-tuning in drug discovery workflows.\n\nFine-tuned from Gemma 2 using diverse instruction-tuning datasets curated from the Therapeutics Data Commons (TDC), the predict variant prioritizes raw prediction performance and efficiency. Unlike chat variants, it is not optimized for multi-turn dialogue or conversational explanations, but delivers stronger performance on focused inference and text-generation tasks.",
    "image_url": null,
    "tags": [
      "Transformers",
      "safetensors",
      "gemma2",
      "Text Generation",
      "therapeutics",
      "drug-development"
    ],
    "likes_count": 0,
    "downloads_count": 1,
    "views_count": 31,
    "source_org": "Google LLC",
    "license": "N.A.",
    "hosted_by": "Google",
    "model_type": "Large Language Models",
    "model_format": "Transformers",
    "visibility": "Open",
    "source_organization": "Google LLC",
    "sector": "Healthcare, Wellness and Family Welfare",
    "updated_at": "2025-09-18T15:55:45+05:30",
    "created_by": "Amrita Kamat",
    "size": "N.A."
  }
]